// Grafana Alloy Configuration for OrbStack Observability
// 
// DATA FLOW: OTLP → Batch → Export to Storage
// PORTS: 4317 (gRPC), 4318 (HTTP), 12345 (internal metrics)
// 
// OPERATIONAL PATTERNS:
// - Normal: <100 req/s, batch size 100-500, <100MB memory
// - Warning: >500 req/s, consider increasing batch size
// - Critical: >1000 req/s, may need multiple Alloy instances
//
// DEBUG: curl http://alloy.local:12345/metrics | grep otlp

// OTLP Receiver for traces and metrics
// METRICS EMITTED:
//   otlp_receiver_accepted_spans - Successful trace ingestion
//   otlp_receiver_refused_spans - Failed ingestion (check logs)
//   otlp_receiver_accepted_metric_points - Metric ingestion rate
//
// AUTHENTICATION (Optional):
// To secure OTLP endpoints with Bearer token authentication:
// 1. Set OTLP_BEARER_TOKEN environment variable in docker-compose
// 2. Uncomment the auth.bearer section below
// 3. Add auth parameter to grpc and http blocks
// 4. Configure clients with: Authorization: Bearer <token>
//
// otelcol.auth.bearer "otlp" {
//   token = env("OTLP_BEARER_TOKEN")
// }
//
otelcol.receiver.otlp "default" {
  grpc {
    endpoint = "0.0.0.0:4317"
    // auth = otelcol.auth.bearer.otlp.handler  // Uncomment to enable auth
  }

  http {
    endpoint = "0.0.0.0:4318"
    // auth = otelcol.auth.bearer.otlp.handler  // Uncomment to enable auth
  }

  output {
    metrics = [otelcol.processor.batch.default.input]
    logs    = [otelcol.processor.batch.default.input]
    traces  = [otelcol.processor.batch.default.input]
  }
}

// Batch processor for efficiency
// TUNING: Increase batch size if seeing high CPU
// Default: 100 items or 10s timeout
// High volume: 500 items or 5s timeout
otelcol.processor.batch "default" {
  output {
    metrics = [otelcol.processor.attributes.genai.input]
    logs    = [otelcol.exporter.loki.default.input]
    traces  = [otelcol.processor.attributes.genai.input]
  }
}

// Gen AI semantic convention processor
// Enriches traces with Gen AI specific attributes and routing
// Maps Ollama done_reason to standard finish_reasons
otelcol.processor.attributes "genai" {
  actions {
    // Ensure Gen AI attributes are properly typed
    key = "gen_ai.usage.input_tokens"
    action = "convert"
    converted_type = "int"
  }
  
  actions {
    key = "gen_ai.usage.output_tokens"
    action = "convert"
    converted_type = "int"
  }
  
  actions {
    // Calculate total tokens for cost tracking
    key = "gen_ai.usage.total_tokens"
    action = "insert"
    from_attribute = "gen_ai.usage.input_tokens"
  }
  
  actions {
    // Map local model costs (inference_ms as proxy for cost)
    key = "gen_ai.resource.local_cost"
    action = "insert"
    from_attribute = "gen_ai.resource.inference_ms"
  }
  
  output {
    metrics = [otelcol.exporter.prometheus.default.input, otelcol.exporter.otlphttp.langfuse.input]
    traces  = [otelcol.exporter.otlp.tempo.input, otelcol.exporter.otlphttp.langfuse.input]
  }
}

// Export metrics to Prometheus
otelcol.exporter.prometheus "default" {
  forward_to = [prometheus.remote_write.local.receiver]
}

// Export traces to Tempo
otelcol.exporter.otlp "tempo" {
  client {
    endpoint = "tempo:4317"
    tls {
      insecure = true
    }
  }
}

// Export logs to Loki
otelcol.exporter.loki "default" {
  forward_to = [loki.write.default.receiver]
}

// Export to Langfuse OTLP endpoint for LLM-specific observability
// Langfuse provides native LLM UI for traces while we keep infrastructure metrics
// AUTH: Requires LANGFUSE_AUTH_BASE64 environment variable
// Format: echo -n "pk-lf-xxx:sk-lf-xxx" | base64
otelcol.exporter.otlphttp "langfuse" {
  client {
    endpoint = env("LANGFUSE_OTLP_ENDPOINT") | "http://langfuse.local:3000/api/public/otel"
    
    headers = {
      "Authorization" = "Basic " + (env("LANGFUSE_AUTH_BASE64") | "")
    }
    
    tls {
      insecure = true
    }
  }
}

// Prometheus remote write to local Prometheus
prometheus.remote_write "local" {
  endpoint {
    url = "http://prometheus:9090/api/v1/write"
  }
}

// Loki write for logs
loki.write "default" {
  endpoint {
    url = "http://loki:3100/loki/api/v1/push"
  }
}

// Scrape Redis exporter (FalkorDB metrics)
// KEY METRICS:
//   redis_keyspace_hits_total - Cache effectiveness (target >85%)
//   redis_memory_used_bytes - Memory consumption (warn >2GB)
//   redis_commands_processed_total - Query load
// NORMAL: 100-500 commands/s, cache hit >85%
// WARNING: >1000 commands/s or cache hit <70%
// CRITICAL: >2000 commands/s or memory >3GB
prometheus.scrape "falkordb" {
  targets = [
    {__address__ = "redis-exporter:9121", job = "falkordb"},
  ]
  forward_to = [prometheus.remote_write.local.receiver]
  scrape_interval = "15s"
}

// Scrape Netdata metrics (Host system monitoring)
// PROVIDES: CPU, memory, disk, network for macOS host
// KEY METRICS:
//   system_cpu_usage - Host CPU (warn >80%)
//   system_ram_usage - Host memory pressure
//   system_io_ops - Disk activity
// NOTE: Requires netdata container running separately
prometheus.scrape "netdata" {
  targets = [
    {__address__ = "netdata-test:19999", job = "netdata"},
  ]
  metrics_path = "/api/v1/allmetrics"
  params = {
    format = ["prometheus"],
  }
  forward_to = [prometheus.remote_write.local.receiver]
  scrape_interval = "30s"
}

// Scrape ClickHouse exporter (Langfuse LLM observability)
// TRACKS: LLM token usage, trace counts, query performance
// KEY METRICS:
//   clickhouse_query_duration - Query latency (warn >1s)
//   clickhouse_table_parts - Data organization
//   clickhouse_insertions_failed - Data loss indicator
// INTEGRATION: Correlate with mcp_tool_invocations for AI insights
prometheus.scrape "clickhouse" {
  targets = [
    {__address__ = "clickhouse-exporter:9116", job = "langfuse"},
  ]
  forward_to = [prometheus.remote_write.local.receiver]
  scrape_interval = "30s"
}

// Docker service discovery for container metrics
// AUTO-DISCOVERS: Containers with prometheus.io labels
// REQUIRED LABELS:
//   prometheus.io.scrape=true - Enable scraping
//   prometheus.io.port=9090 - Metrics port
//   prometheus.io.job=service-name - Job name
// EXAMPLE: Add to your docker-compose service:
//   labels:
//     - prometheus.io.scrape=true
//     - prometheus.io.port=8080
//     - prometheus.io.job=my-api
discovery.docker "containers" {
  host = "unix:///var/run/docker.sock"
  refresh_interval = "30s"
}

// Process Docker discovered targets
discovery.relabel "docker_containers" {
  targets = discovery.docker.containers.targets

  // Keep only containers with monitoring enabled
  rule {
    source_labels = ["__meta_docker_container_label_prometheus_io_scrape"]
    regex = "true"
    action = "keep"
  }

  // Set instance name from container name
  rule {
    source_labels = ["__meta_docker_container_name"]
    target_label = "instance"
  }

  // Set job name from container label or default to container name
  rule {
    source_labels = ["__meta_docker_container_label_prometheus_io_job"]
    target_label = "job"
  }

  // Extract metrics port from label
  rule {
    source_labels = ["__meta_docker_container_label_prometheus_io_port"]
    target_label = "__tmp_port"
  }

  // Construct address from container name and port
  rule {
    source_labels = ["__meta_docker_container_name", "__tmp_port"]
    separator = ":"
    target_label = "__address__"
    replacement = "${1}:${2}"
  }
}

// Scrape discovered Docker containers
prometheus.scrape "docker" {
  targets = discovery.relabel.docker_containers.output
  forward_to = [prometheus.remote_write.local.receiver]
  scrape_interval = "30s"
}

// Collect logs from Docker containers
loki.source.docker "containers" {
  host = "unix:///var/run/docker.sock"
  targets = discovery.docker.containers.targets
  forward_to = [loki.write.default.receiver]
  relabel_rules = discovery.relabel.docker_containers.rules
}

// Standalone cAdvisor for container monitoring
// METRICS: container_* (CPU, memory, disk, network)
// NORMAL: CPU <50%, Memory <1GB per container
// WARNING: CPU >70% or Memory >2GB  
// CRITICAL: CPU >90% or Memory >3GB
// KEY METRICS:
//   container_cpu_usage_seconds_total - CPU utilization
//   container_memory_usage_bytes - Memory consumption
//   container_network_receive_bytes_total - Network ingress
//   container_network_transmit_bytes_total - Network egress
//   container_last_seen - Container health/liveness
prometheus.scrape "cadvisor" {
  targets = [
    {__address__ = "cadvisor:8080", job = "cadvisor"},
  ]
  forward_to = [prometheus.remote_write.local.receiver]
  scrape_interval = "30s"
  metrics_path = "/metrics"
}

// Self-monitoring
// TRACKS: Alloy's own resource usage and pipeline health
// KEY METRICS:
//   alloy_config_load_success - Config validity
//   alloy_component_controller_running - Pipeline status
//   process_resident_memory_bytes - Alloy memory usage
// NORMAL: <100MB memory, all components running
// WARNING: >200MB memory - check for backpressure
// CRITICAL: Components failing - check logs
prometheus.exporter.self "alloy" {
}

prometheus.scrape "alloy_self" {
  targets = prometheus.exporter.self.alloy.targets
  forward_to = [prometheus.remote_write.local.receiver]
  scrape_interval = "30s"
}