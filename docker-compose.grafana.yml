version: '3.8'

# Grafana Observability Stack for AI Agent Monitoring
# Quick Start: docker compose -f docker-compose.grafana.yml up -d
# Access: http://grafana.local (admin/admin)
# 
# OPERATIONAL NOTES:
# - All services auto-restart on failure
# - OrbStack provides *.local domains automatically
# - Metrics retention: Prometheus 90d, Tempo 30d, Loki 3d
# - Total memory usage should stay under 4GB normal, 8GB peak

services:
  # Grafana for visualization
  # METRICS: None (UI only)
  # NORMAL: Dashboard loads <2s
  # WARNING: >5s load time - check query complexity
  # CRITICAL: Timeouts - reduce panel count or time range
  grafana:
    image: grafana/grafana:latest
    container_name: grafana-main
    labels:
      - dev.orbstack.domains=grafana.local
    ports:
      - "3001:3000"
    environment:
      - GF_SECURITY_ADMIN_USER=admin
      - GF_SECURITY_ADMIN_PASSWORD=admin
      # - GF_AUTH_ANONYMOUS_ENABLED=true
      # - GF_AUTH_ANONYMOUS_ORG_ROLE=Admin
      - GF_FEATURE_TOGGLES_ENABLE=traceToMetrics,traceToLogs
    volumes:
      - grafana-storage:/var/lib/grafana
      - ./dashboards:/etc/grafana/provisioning/dashboards
      - ./config/datasources.yml:/etc/grafana/provisioning/datasources/datasources.yml
    networks:
      - observability
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "wget", "--quiet", "--tries=1", "--spider", "http://localhost:3000/api/health"]
      interval: 30s
      timeout: 10s
      retries: 3

  # Prometheus for metrics storage
  # METRICS: prometheus_tsdb_* (self-monitoring)
  # NORMAL: <2GB storage, <100MB memory
  # WARNING: >5GB storage - consider reducing retention
  # CRITICAL: >10GB storage - urgent cleanup needed
  # KEY QUERIES:
  #   up - Service health checks
  #   rate(metric_total[5m]) - Rate calculations
  #   histogram_quantile() - Latency percentiles
  prometheus:
    image: prom/prometheus:latest
    container_name: prometheus-local
    labels:
      - dev.orbstack.domains=prometheus.local
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.enable-remote-write-receiver'
      - '--enable-feature=exemplar-storage'
      - '--storage.tsdb.retention.time=90d'
    volumes:
      - ./config/prometheus.yml:/etc/prometheus/prometheus.yml
      - prometheus-data:/prometheus
    networks:
      - observability
    restart: unless-stopped

  # Tempo for distributed tracing  
  # METRICS: tempo_* (throughput, latency)
  # NORMAL: <1GB storage, trace ingestion <1000/s
  # WARNING: Trace search >5s - too many traces
  # CRITICAL: Storage full - reduce retention
  # INTEGRATION: Links from logs/metrics to traces via trace_id
  tempo:
    image: grafana/tempo:latest
    container_name: tempo-local
    labels:
      - dev.orbstack.domains=tempo.local
    command: [ "-config.file=/etc/tempo.yaml" ]
    volumes:
      - ./config/tempo.yaml:/etc/tempo.yaml
      - tempo-data:/var/tempo
    networks:
      - observability
    restart: unless-stopped

  # Loki for logs
  # METRICS: loki_* (ingestion rate, query performance)
  # NORMAL: <500MB storage, <1000 logs/s
  # WARNING: Ingestion lag >30s - check pipeline
  # CRITICAL: Queries timing out - reduce time range
  # TIP: Use LogQL for structured queries: {job="docker"} |= "error"
  loki:
    image: grafana/loki:latest
    container_name: loki-local
    labels:
      - dev.orbstack.domains=loki.local
    command: -config.file=/etc/loki/local-config.yaml
    volumes:
      - loki-data:/loki
    networks:
      - observability
    restart: unless-stopped

  # Grafana Alloy for collection
  # METRICS: otlp_receiver_* (request count, latency)
  # NORMAL: <100MB memory, <1% CPU
  # WARNING: Dropped spans/metrics - check batch size
  # CRITICAL: Pipeline blocked - restart required
  # PORTS:
  #   4317: OTLP gRPC (preferred for high volume)
  #   4318: OTLP HTTP (for browser/curl testing)
  #   12345: Internal metrics endpoint
  alloy:
    image: grafana/alloy:latest
    container_name: grafana-alloy
    labels:
      - dev.orbstack.domains=alloy.local
    ports:
      - "4317:4317"     # OTLP gRPC for external services
      - "4318:4318"     # OTLP HTTP for external services
    volumes:
      - ./config/alloy-config.alloy:/etc/alloy/config.alloy
      - /var/run/docker.sock:/var/run/docker.sock:ro  # For Docker service discovery
      - /sys:/sys:ro  # For cAdvisor to read cgroup stats
      - /var/lib/docker:/var/lib/docker:ro  # For cAdvisor to read container info
    command:
      - run
      - /etc/alloy/config.alloy
      - --server.http.listen-addr=0.0.0.0:12345
      - --storage.path=/tmp/alloy
    environment:
      - HOSTNAME=alloy
    networks:
      - observability
      - langfuse-prod_default  # Cross-network attachment for Langfuse monitoring
    restart: unless-stopped
    depends_on:
      - prometheus
      - tempo
      - loki

  # Redis Exporter for FalkorDB
  # METRICS: redis_* (memory, commands, clients)
  # NORMAL: Cache hit rate >85%, memory <2GB
  # WARNING: Hit rate 70-85% - cache warming needed
  # CRITICAL: Hit rate <70% - performance degradation
  # KEY METRICS:
  #   redis_keyspace_hits_total - Cache effectiveness
  #   redis_memory_used_bytes - GraphRAG memory usage
  #   redis_commands_processed_total - Query volume
  redis-exporter:
    image: oliver006/redis_exporter:latest
    container_name: redis-exporter
    labels:
      - dev.orbstack.domains=redis-exporter.local
    environment:
      - REDIS_ADDR=falkordb:6379  # FalkorDB container
      - REDIS_EXPORTER_CHECK_KEYS=shared_knowledge_graph:*
      - REDIS_EXPORTER_CHECK_SINGLE_KEYS=shared_knowledge_graph
      - REDIS_EXPORTER_INCL_SYSTEM_METRICS=true
    command:
      - --include-system-metrics
      - --check-keys=shared_knowledge_graph:*
    networks:
      - observability
    restart: unless-stopped

  # ClickHouse Exporter for Langfuse metrics
  # METRICS: clickhouse_* (queries, connections, memory)
  # NORMAL: Query time <100ms, <1GB memory
  # WARNING: Slow queries >1s - check indexes
  # CRITICAL: Connection pool exhausted
  # LANGFUSE TRACKING:
  #   LLM token usage, trace counts, latency percentiles
  clickhouse-exporter:
    image: f1yegor/clickhouse-exporter
    container_name: clickhouse-exporter
    labels:
      - dev.orbstack.domains=clickhouse-exporter.local
    environment:
      - CLICKHOUSE_USER=clickhouse
      - CLICKHOUSE_PASSWORD=clickhouse
    command:
      - -scrape_uri=http://langfuse-prod-clickhouse-1:8123/
    networks:
      - observability
    restart: unless-stopped

  # cAdvisor for container monitoring
  # METRICS: container_* (CPU, memory, network, disk)
  # NORMAL: CPU <50%, Memory <1GB per container
  # WARNING: CPU >70% or Memory >2GB
  # CRITICAL: CPU >90% or Memory >3GB or container restarts
  # KEY METRICS:
  #   container_cpu_usage_seconds_total - CPU utilization rate
  #   container_memory_usage_bytes - Current memory usage
  #   container_network_receive_bytes_total - Network ingress
  #   container_network_transmit_bytes_total - Network egress
  cadvisor:
    image: gcr.io/cadvisor/cadvisor:latest
    container_name: cadvisor
    labels:
      - dev.orbstack.domains=cadvisor.local
    privileged: true
    devices:
      - /dev/kmsg
    volumes:
      - /:/rootfs:ro
      - /var/run:/var/run:ro
      - /sys:/sys:ro
      - /var/lib/docker:/var/lib/docker:ro
      - /dev/disk:/dev/disk:ro
    ports:
      - "8080:8080"
    networks:
      - observability
      - langfuse-prod_default  # Cross-network for Langfuse monitoring
    restart: unless-stopped
    command:
      - --docker_only=true
      - --housekeeping_interval=30s
      - --max_housekeeping_interval=60s
      - --storage_duration=5m0s

# Networks configuration
# The observability network allows all services to communicate
# OrbStack automatically provides DNS resolution via container names
networks:
  observability:
    name: observability
    driver: bridge
  langfuse-prod_default:
    external: true  # External network from Langfuse stack

# Volume configuration
# IMPORTANT: These volumes persist data between container restarts
# To completely reset: docker compose down -v
# Size estimates (after 30 days):
#   grafana-storage: <100MB (dashboards, settings)
#   prometheus-data: 5-10GB (90 day retention)
#   tempo-data: 1-3GB (30 day retention)
#   loki-data: 500MB-1GB (3 day retention)
volumes:
  grafana-storage:
    driver: local
  prometheus-data:
    driver: local
  tempo-data:
    driver: local
  loki-data:
    driver: local